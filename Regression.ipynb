{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# *Regression Assignment*\n",
        "1. What is Simple Linear Regression?\n",
        "-> Simple Linear Regression is a statistical method that models the linear relationship between a single independent variable (predictor) and a single dependent variable (response).\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "-> The key assumptions are: linearity, independence of errors, homoscedasticity (constant variance of errors), and normality of errors.\n",
        "3.  What does the coefficient m represent in the equation Y=mX+c?\n",
        "->The coefficient 'm' represents the slope of the regression line, indicating the change in the dependent variable (Y) for a one-unit increase in the independent variable (X).\n",
        "4.  What does the intercept c represent in the equation Y=mX+c?\n",
        "->The intercept 'c' represents the value of the dependent variable (Y) when the independent variable (X) is zero.\n",
        "\n",
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "-> The slope 'm' is calculated as the covariance between X and Y divided by the variance of X.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "-> The least squares method aims to find the best-fitting regression line by minimizing the sum of the squared differences between the observed values of the dependent variable and the values predicted by the regression line.\n",
        "\n",
        "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
        "-> The coefficient of determination (R²) represents the proportion of the variance in the dependent variable that is predictable from the independent variable. It ranges from 0 to 1, where a higher value indicates a better fit.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "-> Multiple Linear Regression is a statistical method that models the linear relationship between two or more independent variables (predictors) and a single dependent variable (response).\n",
        "\n",
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "-> Simple Linear Regression involves one independent variable, while Multiple Linear Regression involves two or more independent variables.\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "-> The key assumptions are: linearity, independence of errors, homoscedasticity, normality of errors, and no perfect multicollinearity (no perfect linear relationship between independent variables).\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "-> Heteroscedasticity is the condition where the variance of the error terms is not constant across all levels of the independent variables. It can lead to inefficient and biased estimates of the regression coefficients and unreliable hypothesis tests.\n",
        "\n",
        "12.  How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "-> Techniques to address multicollinearity include: removing one or more of the highly correlated variables, collecting more data, or using dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
        "\n",
        "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
        "-> Common techniques include: dummy coding (creating binary variables for each category), effect coding, and orthogonal coding.\n",
        "\n",
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "-> Interaction terms allow the effect of one independent variable on the dependent variable to depend on the level of another independent variable. They capture synergistic or antagonistic effects between predictors.\n",
        "\n",
        "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "-> In Simple Linear Regression, the intercept is the value of Y when X is zero. In Multiple Linear Regression, the intercept is the value of Y when all independent variables are zero. Its practical interpretation can be more nuanced depending on whether setting all predictors to zero is meaningful.\n",
        "\n",
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "-> The slope indicates the direction and magnitude of the relationship between an independent variable and the dependent variable. It quantifies how much the dependent variable is expected to change for a one-unit change in the independent variable, directly influencing predictions.\n",
        "\n",
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "-> The intercept provides a baseline value for the dependent variable when all independent variables are zero. It helps to anchor the regression line and can offer a starting point for understanding the relationship, although its practical meaning depends on the context of the variables.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of model performance?\n",
        "-> R² only measures the proportion of variance explained and doesn't indicate if the model is correctly specified, if there are biased estimates, or if the model will generalize well to new data. It can also increase with the addition of more variables, even if they don't meaningfully improve the model.\n",
        "\n",
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "-> A large standard error for a regression coefficient suggests that the estimated coefficient is not very precise. There is a wide range of plausible values for the true coefficient, making it difficult to be confident about the magnitude and even the direction of the effect of the predictor on the response.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "-> Heteroscedasticity can be identified in residual plots by observing a non-constant variance of residuals as the predicted values increase (e.g., a funnel shape). It's important to address because it violates the assumptions of regression, leading to unreliable standard errors, hypothesis tests, and confidence intervals.\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
        "-> A high R² and a low adjusted R² suggest that the model includes one or more independent variables that do not significantly contribute to explaining the variance in the dependent variable. The adjusted R² penalizes the inclusion of unnecessary predictors.\n",
        "\n",
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "-> Scaling variables can help to: prevent variables with larger ranges from unduly influencing the model, improve the convergence speed of optimization algorithms, and make the interpretation of coefficients more comparable, especially when interaction terms are involved.\n",
        "\n",
        "23. What is polynomial regression?\n",
        "-> Polynomial regression is a form of regression analysis in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth-degree polynomial.\n",
        "\n",
        "24.  How does polynomial regression differ from linear regression?\n",
        "-> Linear regression models a linear relationship, while polynomial regression models a non-linear relationship using polynomial terms.\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "-> Polynomial regression is used when the relationship between the variables appears to be curvilinear rather than linear.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "-> The general equation for polynomial regression of degree 'n' with one independent variable is:\n",
        "Y=β0​ +β1X+β2X^2+⋯+βnX^n +ϵ\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "-> Yes, polynomial regression can be extended to multiple independent variables, resulting in more complex models with terms involving powers and interactions of the predictors.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "-> Limitations include: overfitting the data with high-degree polynomials, difficulty in interpreting high-order terms, and potential for poor extrapolation beyond the range of the observed data.\n",
        "\n",
        "29.  What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "-> Methods include: examining R² and adjusted R², using statistical tests (like F-tests to compare models), analyzing residual plots for patterns, and using cross-validation techniques.\n",
        "\n",
        "30. Why is visualization important in polynomial regression?\n",
        "-> Visualization, such as plotting the fitted polynomial curve against the data points and examining residual plots, is crucial for understanding the shape of the relationship and identifying potential issues like overfitting or poor fit in certain regions of the data.\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "-> Polynomial regression can be implemented in Python using libraries like scikit-learn by creating polynomial features from the original independent variable(s) and then fitting a linear regression model to these transformed features."
      ],
      "metadata": {
        "id": "Gk1HTtWappqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZVrE6TspToh"
      },
      "outputs": [],
      "source": []
    }
  ]
}